import os
import re
import io
import zipfile
import subprocess
from dataclasses import dataclass
from typing import List, Tuple, Optional, Dict

import streamlit as st
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel

# (ì„ íƒ) ê°€ìš© ì‹œ ì‚¬ìš©
try:
    import olefile  # .hwp OLE ì»¨í…Œì´ë„ˆ ì ‘ê·¼ìš©
except Exception:
    olefile = None

# PDF ì¶”ì¶œ
from pdfminer.high_level import extract_pages
from pdfminer.layout import LTTextContainer, LTTextBox, LTTextLine


# ---------------------------
# í…ìŠ¤íŠ¸ ì¶”ì¶œ ìœ í‹¸
# ---------------------------

def extract_text_pdf_per_page(path: str) -> List[Tuple[int, str]]:
    """PDFì—ì„œ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    pages = []
    try:
        for i, layout in enumerate(extract_pages(path), start=1):
            texts = []
            for el in layout:
                if isinstance(el, (LTTextContainer, LTTextBox, LTTextLine)):
                    try:
                        texts.append(el.get_text())
                    except Exception:
                        pass
            page_text = "".join(texts).strip()
            if page_text:
                pages.append((i, page_text))
    except Exception as e:
        st.warning(f"[PDF ì½ê¸° ì‹¤íŒ¨] {path} - {e}")
    return pages


def extract_text_hwpx(path: str) -> str:
    """
    HWPX(ì••ì¶•ëœ XML íŒ¨í‚¤ì§€)ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ(ë²”ìš©).
    êµ¬ì¡°ë¥¼ ê¹Šì´ í•´ì„í•˜ì§€ ì•Šê³  ëª¨ë“  XML í…ìŠ¤íŠ¸ ë…¸ë“œë¥¼ ìˆ˜ì§‘.
    """
    text_nodes = []
    try:
        with zipfile.ZipFile(path, 'r') as zf:
            # í…ìŠ¤íŠ¸ ë³¸ë¬¸ì´ ë“¤ì–´ìˆëŠ” ì„¹ì…˜/ë³¸ë¬¸ XMLë“¤ì„ ìš°ì„ 
            candidates = [n for n in zf.namelist() if n.lower().endswith(".xml")]
            for name in candidates:
                try:
                    data = zf.read(name)
                    # lxmlì´ ìˆìœ¼ë©´ ì‚¬ìš©(ì†ë„â†‘), ì—†ìœ¼ë©´ í‘œì¤€ libë¡œ ì²˜ë¦¬
                    try:
                        from lxml import etree as ET  # type: ignore
                    except Exception:
                        import xml.etree.ElementTree as ET  # fallback
                    root = ET.fromstring(data)
                    # ëª¨ë“  í…ìŠ¤íŠ¸ ë…¸ë“œë¥¼ ìˆ˜ì§‘
                    for elem in root.iter():
                        t = (elem.text or "").strip()
                        if t:
                            text_nodes.append(t)
                except Exception:
                    continue
    except Exception as e:
        st.warning(f"[HWPX ì½ê¸° ì‹¤íŒ¨] {path} - {e}")
    return "\n".join(text_nodes).strip()


def extract_text_hwp_via_hwp5txt(path: str) -> Optional[str]:
    """
    hwp5txt CLIê°€ ìˆìœ¼ë©´ ê·¸ê²ƒìœ¼ë¡œ .hwp í…ìŠ¤íŠ¸ ì¶”ì¶œ.
    ë¦¬ëˆ…ìŠ¤ ë°°í¬íŒ(hwp5-tools)ì—ì„œ ì£¼ë¡œ ì œê³µ. ì—†ìœ¼ë©´ None.
    """
    try:
        out = subprocess.check_output(["hwp5txt", path], stderr=subprocess.STDOUT, timeout=20)
        return out.decode("utf-8", errors="ignore")
    except Exception:
        return None


def extract_text_hwp_best_effort(path: str) -> str:
    """
    .hwp(5.x) ë² ìŠ¤íŠ¸ì—í¬íŠ¸ ì¶”ì¶œ:
    1) hwp5txt ì‚¬ìš© ì‹œë„
    2) olefileë¡œ BodyText/Section* ìŠ¤íŠ¸ë¦¼ ê¸ì–´ì„œ ê°€ì‹œ í…ìŠ¤íŠ¸ í›„ë³´ ì¶”ì¶œ(ì™„ë²½ X)
    """
    # 1) hwp5txtê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
    t = extract_text_hwp_via_hwp5txt(path)
    if isinstance(t, str) and t.strip():
        return t

    # 2) olefileë¡œ ì›ì‹œ ë°”ì´íŠ¸ì—ì„œ í•œê¸€/ì˜ë¬¸ ê°€ì‹œë¬¸ìë§Œ ì¶”ì¶œí•˜ëŠ” ê°„ì´ ë²„ì „
    if olefile is None:
        st.info(f"[ì•ˆë‚´] .hwp ì¶”ì¶œì„ ì›í™œíˆ í•˜ë ¤ë©´ 'hwp5txt' ì„¤ì¹˜ ë˜ëŠ” 'olefile' ëª¨ë“ˆ ì„¤ì¹˜ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤: {path}")
        return ""

    try:
        of = olefile.OleFileIO(path)
        # BodyText ì•ˆì˜ Section* ìŠ¤íŠ¸ë¦¼ë“¤ ëŒ€ìƒ
        sections = [s for s in of.listdir() if len(s) >= 2 and s[0] == 'BodyText' and s[1].startswith('Section')]
        texts = []
        for sect in sections:
            try:
                data = of.openstream(sect).read()
                # ì••ì¶•/í¬ë§·ì„ ì™„ë²½íˆ í•´ì„í•˜ì§„ ì•Šê³ , ê°€ì‹œ ë¬¸ìì—´ íŒíŠ¸ë§Œ ì¶”ì¶œ(ì„ì‹œ)
                # í•œê¸€/ì˜ë¬¸/ìˆ«ì/ê³µë°±/êµ¬ë‘ì  ë“±ì„ ë½‘ì•„ë‚´ëŠ” ì •ê·œì‹
                guess = re.findall(r"[ \t\r\nA-Za-z0-9ê°€-í£ã„±-ã…ã…-ã…£.,:;!?()\-_/\\\[\]{}\"'`~@#$%^&*=+<>|]+", data)
                chunk = b" ".join(guess).decode("utf-8", errors="ignore")
                # ë¹„ì •ìƒ ë°˜ë³µ ê³µë°± ì¶•ì†Œ
                chunk = re.sub(r"\s{2,}", " ", chunk)
                texts.append(chunk)
            except Exception:
                continue
        of.close()
        return "\n".join(texts).strip()
    except Exception as e:
        st.warning(f"[HWP ì½ê¸° ì‹¤íŒ¨] {path} - {e}")
        return ""


def read_file_to_units(path: str) -> List[Tuple[str, Optional[int]]]:
    """
    íŒŒì¼ì„ 'ê²€ìƒ‰ ê°€ëŠ¥í•œ ë‹¨ìœ„(text, page)' ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜.
    - PDF: í˜ì´ì§€ë³„ ë‹¨ìœ„
    - HWP/HWPX: ë¬¸ì„œ ì „ì²´(í˜ì´ì§€ ì •ë³´ ë¶ˆëª…) 1ê°œ ë‹¨ìœ„
    """
    ext = os.path.splitext(path)[1].lower()
    if ext == ".pdf":
        pages = extract_text_pdf_per_page(path)
        return [(txt, pno) for (pno, txt) in pages if txt.strip()]
    elif ext == ".hwpx":
        txt = extract_text_hwpx(path)
        return [(txt, None)] if txt.strip() else []
    elif ext == ".hwp":
        txt = extract_text_hwp_best_effort(path)
        return [(txt, None)] if txt.strip() else []
    else:
        return []


# ---------------------------
# ìƒ‰ì¸/ê²€ìƒ‰
# ---------------------------

@dataclass
class Passage:
    doc_path: str
    page: Optional[int]
    text: str


def split_into_chunks(text: str, chunk_size: int = 800, overlap: int = 150) -> List[str]:
    """
    ë¬¸ë‹¨ ê¸°ì¤€ìœ¼ë¡œ í•©ì¹˜ë˜, ìµœì¢…ì ìœ¼ë¡œëŠ” ë¬¸ì ê¸¸ì´ë¡œ ìŠ¬ë¼ì´ì‹±.
    í•œêµ­ì–´ì—ì„œëŠ” char n-gram ê¸°ë°˜ ê²€ìƒ‰ì´ ì˜ ë™ì‘í•˜ë¯€ë¡œ ë¬¸ì ë‹¨ìœ„ê°€ ì•ˆì „.
    """
    text = re.sub(r"\r\n|\r", "\n", text)
    paras = [p.strip() for p in text.split("\n") if p.strip()]
    buf, chunks = "", []
    for p in paras:
        # ìƒˆ ë¬¸ë‹¨ ì¶”ê°€ ì‹œ ê¸¸ì´ ì²´í¬
        if len(buf) + len(p) + 1 <= chunk_size:
            buf = (buf + "\n" + p).strip() if buf else p
        else:
            if buf:
                chunks.append(buf)
            # ì˜¤ë²„ë© ì ìš©
            if overlap > 0 and chunks:
                tail = chunks[-1][-overlap:]
                buf = (tail + "\n" + p).strip()
            else:
                buf = p
    if buf:
        chunks.append(buf)

    # í˜¹ì‹œ ë„ˆë¬´ ê¸´ ë‹¨ë½ì´ ë“¤ì–´ì™”ì„ ë•Œ ì¶”ê°€ ìŠ¬ë¼ì´ìŠ¤
    final = []
    for c in chunks:
        if len(c) <= chunk_size:
            final.append(c)
        else:
            i = 0
            while i < len(c):
                final.append(c[i:i+chunk_size])
                i += (chunk_size - overlap) if overlap < chunk_size else chunk_size
    return final


def build_index(root_dir: str, exts: Tuple[str, ...] = (".pdf", ".hwp", ".hwpx"),
                chunk_size: int = 800, overlap: int = 150) -> Tuple[List[Passage], TfidfVectorizer]:
    passages: List[Passage] = []
    # íŒŒì¼ ìˆ˜ì§‘
    file_list = []
    for r, _, files in os.walk(root_dir):
        for fn in files:
            if os.path.splitext(fn)[1].lower() in exts:
                file_list.append(os.path.join(r, fn))

    progress = st.progress(0.0, text="ë¬¸ì„œë¥¼ ì½ëŠ” ì¤‘...")
    total = max(1, len(file_list))

    for idx, path in enumerate(sorted(file_list)):
        units = read_file_to_units(path)
        for (txt, page) in units:
            if not txt or len(txt.strip()) == 0:
                continue
            for c in split_into_chunks(txt, chunk_size=chunk_size, overlap=overlap):
                passages.append(Passage(doc_path=path, page=page, text=c))
        progress.progress((idx + 1) / total, text=f"ìƒ‰ì¸ ì¤‘... ({idx+1}/{total})")

    progress.empty()

    # í•œêµ­ì–´ ì¹œí™”: char n-gram ê¸°ë°˜ TF-IDF
    vectorizer = TfidfVectorizer(
        analyzer="char_wb",
        ngram_range=(3, 5),
        min_df=1,
        max_df=1.0
    )
    corpus = [p.text for p in passages] if passages else ["(empty)"]
    X = vectorizer.fit_transform(corpus)

    # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
    st.session_state["index_matrix"] = X
    st.session_state["vectorizer"] = vectorizer
    st.session_state["passages"] = passages
    st.session_state["root_dir"] = root_dir

    return passages, vectorizer


def ensure_index(root_dir: str, chunk_size: int = 1000, overlap: int = 120) -> bool:
    """ì„¸ì…˜ ìƒíƒœì— ì¸ë±ìŠ¤ê°€ ì—†ê±°ë‚˜ ê²½ë¡œ/íŒŒë¼ë¯¸í„°ê°€ ë°”ë€Œë©´ ìƒˆë¡œ ìƒì„±"""
    need_rebuild = False
    if "passages" not in st.session_state:
        need_rebuild = True
    else:
        if st.session_state.get("root_dir") != root_dir:
            need_rebuild = True
        if st.session_state.get("chunk_size") != chunk_size or st.session_state.get("overlap") != overlap:
            need_rebuild = True

    if need_rebuild:
        passages, _ = build_index(root_dir)
        st.session_state["chunk_size"] = chunk_size
        st.session_state["overlap"] = overlap
        if not passages:
            st.warning("ì¸ë±ìŠ¤ì— ì¶”ê°€ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. í´ë”/í™•ì¥ìë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            return False
    return True


def search(query: str, top_k: int = 10) -> List[Tuple[float, Passage]]:
    vectorizer: TfidfVectorizer = st.session_state["vectorizer"]
    X = st.session_state["index_matrix"]
    passages: List[Passage] = st.session_state["passages"]

    qv = vectorizer.transform([query])
    sims = linear_kernel(qv, X).flatten()  # cosine ìœ ì‚¬ë„
    # top_k ì¸ë±ìŠ¤ ì¶”ì¶œ
    top_idx = sims.argsort()[::-1][:top_k]
    results = [(float(sims[i]), passages[i]) for i in top_idx]
    return results


def highlight(text: str, query: str) -> str:
    """ê°„ë‹¨ í•˜ì´ë¼ì´íŠ¸(ëŒ€ì†Œë¬¸ì ë¬´ì‹œ). í•œêµ­ì–´ë„ ë¶€ë¶„ ì¼ì¹˜."""
    if not query.strip():
        return text
    try:
        pattern = re.escape(query.strip())
        return re.sub(pattern, lambda m: f"**{m.group(0)}**", text, flags=re.IGNORECASE)
    except Exception:
        return text


# ---------------------------
# UI
# ---------------------------

st.set_page_config(page_title="ë¬¸ì„œ ê²€ìƒ‰ (PDF/HWP/HWPX) â€” LLM ì—†ëŠ” RAG", layout="wide")

st.title("ğŸ” ë¬¸ì„œ ê²€ìƒ‰ (PDF/HWP/HWPX")
st.caption("í´ë” ë‚´ ë¬¸ì„œë¥¼ ìƒ‰ì¸í•˜ê³  TF-IDFë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤. ê²°ê³¼ì—ëŠ” ê·¼ê±°(íŒŒì¼ëª…/í˜ì´ì§€)ë¥¼ í•¨ê»˜ í‘œì‹œí•©ë‹ˆë‹¤.")

root_dir = os.getcwd() + r"\ragìš©íŒŒì¼"
chunk_size = 800
overlap = 150
top_k = 5


# ì²˜ìŒ ì§„ì…/ì¬ìƒ‰ì¸
if "passages" not in st.session_state:
    ensure_index(root_dir, chunk_size, overlap)


# ê²€ìƒ‰ ì˜ì—­
query = st.text_input("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”", value="", placeholder="ì˜ˆ) ì „ê¸°ì•ˆì „ ì ê²€ ì£¼ê¸°, ì§€ì—° ë°°ìƒ ê¸°ì¤€, ê³„ì•½ í•´ì§€ ì¡°í•­ ...")
search_btn = st.button("ê²€ìƒ‰ ì‹¤í–‰")

if search_btn and query.strip():
    if "passages" not in st.session_state:
        st.warning("ë¨¼ì € ìƒ‰ì¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”.")
    else:
        results = search(query, top_k=top_k)
        if not results:
            st.info("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.write(f"**ê²€ìƒ‰ ê²°ê³¼ {len(results)}ê±´**")
            for score, passage in results:
                src = os.path.relpath(passage.doc_path, start=root_dir) if st.session_state.get("root_dir") else passage.doc_path
                where = f"{src}" + (f" Â· p.{passage.page}" if passage.page else "")
                with st.container(border=True):
                    st.markdown(f"**ê·¼ê±°:** `{where}`  |  ìœ ì‚¬ë„: `{score:.3f}`")
                    st.markdown(highlight(passage.text, query))

# í•˜ë‹¨ ì•ˆë‚´
with st.expander("â„¹ï¸ .hwp ì§€ì› ê´€ë ¨ ì•ˆë‚´"):
    st.markdown(
        "- ìµœì : ì‹œìŠ¤í…œì— `hwp5txt`(hwp5-tools)ê°€ ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´ ê³ í’ˆì§ˆ í…ìŠ¤íŠ¸ ì¶”ì¶œì„ ì‹œë„í•©ë‹ˆë‹¤.\n"
        "- ëŒ€ì•ˆ: `olefile`ë§Œ ìˆëŠ” ê²½ìš° ë‚´ë¶€ ìŠ¤íŠ¸ë¦¼ì—ì„œ ê°€ì‹œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì • ì¶”ì¶œ(ì™„ë²½í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ).\n"
        "- ì •í™•ë„ê°€ ì¤‘ìš”í•œ .hwpëŠ” **HWPXë¡œ ì €ì¥**í•˜ê±°ë‚˜ **PDFë¡œ ë³€í™˜** í›„ ìƒ‰ì¸ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
    )


